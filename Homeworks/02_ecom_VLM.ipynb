{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ДЗ №2 Оценка качества Visual Language Model в задачах e-com\n",
        "\n",
        "Вы работаете в R&D-подразделении крупного e-commerce-сервиса.  \n",
        "В компании уже используются разные модели для:\n",
        "- классификации товаров по категориям,\n",
        "- генерации описаний и заголовков,\n",
        "- извлечения признаков для поиска и рекомендаций.\n",
        "\n",
        "Хочется заменить этот зоопарк моделей на **единую мультимодальную модель (VLM)**,  \n",
        "которая могла бы решать несколько задач сразу.\n",
        "\n",
        "Ваша задача — построить **мини-бенчмарки** для оценки мультимодальных моделей  на реальном датасете товаров, провести сравнение моделей и сделать вывод,  \n",
        "какая модель лучше подходит для внедрения в e-commerce-сценарий.\n",
        "\n",
        "\n",
        "## Исходные данные\n",
        "\n",
        "В рамках задания предлагается использовать датасет [**AMAZON-Products-2023**](https://huggingface.co/datasets/milistu/AMAZON-Products-2023),  \n",
        "содержащий данные о товарах (изображения, названия, описания, категории, цены и пр.).  \n",
        "\n",
        "Размер датасета — более 100 000 объектов,  \n",
        "поэтому вам необходимо выбрать **подмножество 1000–2000 объектов** для экспериментов. Какие именно объекты выбирать - необходимо определить Вам, чтобы бенчмарки были как можно более репрезентативными.\n",
        "\n",
        "---\n",
        "\n",
        "## Часть 1. Классификация / регрессия\n",
        "\n",
        "1. Сформулируйте задачу, которую можно оценить классическими метриками  \n",
        "   (например, классификация категории товара, предсказание диапазона цены, определение бренда и т.д.).\n",
        "\n",
        "2. Подготовьте данные (изображения, названия, метки) в удобном для инференса формате.\n",
        "\n",
        "3. Примените 2–3 мультимодальные модели (например, Qwen2.5-VL, LLaVA-OneVision, InternVL). Обратите внимание на время инференса каждой модели, оцените необходимые ресурсы заранее, чтобы не упереться по времени в дедлайн.\n",
        "\n",
        "4. Рассчитайте **эвристики и метрики** для оценки качества модели:\n",
        "   - accuracy, F1, MSE, cosine similarity, и др.\n",
        "   - опишите, почему вы выбрали именно эти метрики.\n",
        "\n",
        "5. Сделайте **анализ результатов**:\n",
        "   - где модели ошибаются;\n",
        "   - какие категории / признаки наиболее сложны;\n",
        "   - какие факторы влияют на качество.\n",
        "\n",
        "---\n",
        "\n",
        "## Часть 2. Генерация названий и описаний товаров\n",
        "\n",
        "1. Поставьте задачу генерации:\n",
        "   - сгенерировать **название** по изображению/описанию;\n",
        "   - сгенерировать **описание** по изображению/категории.\n",
        "\n",
        "2. Сравните результаты моделей:\n",
        "   - автоматические метрики: BLEU, ROUGE, CLIPTextSim;\n",
        "   - использование **LLM-as-a-Judge** — примените языковую модель как «судью»,  \n",
        "     которая сравнивает сгенерированный и эталонный текст. Вы можете самостоятельно выбрать данную модель.\n",
        "\n",
        "3. Оцените, какие модели лучше справляются с задачей описания,  \n",
        "   а какие — с генерацией коротких названий, как влияет использование разных модальностей на результат.\n",
        "\n",
        "---\n",
        "\n",
        "## Оценка и фреймворки\n",
        "\n",
        "- Основная часть — ваша собственная реализация метрик и LLM-Judge.  \n",
        "- **Дополнительные баллы:**  \n",
        "  если вы интегрируете один из существующих фреймворков для оценки качества:\n",
        "  - [`LMMs-Eval`](https://github.com/EvolvingLMMs-Lab/lmms-eval)  \n",
        "  - или другой open-source тулкит для мультимодальной оценки.\n",
        "\n",
        "---\n",
        "\n",
        "## Формат работы\n",
        "\n",
        "1. Выполняйте шаги в ноутбуке. Если часть реализована в отдельных скриптах или нескольких ноутбуках - загрузите в качестве решения все артефакты.\n",
        "2. Все визуализации (графики, таблицы, примеры товаров) должны быть в ноутбуке.  \n",
        "3. Каждый блок должен содержать краткое пояснение в Markdown.  \n",
        "4. Код должен быть воспроизводимым (укажите seed, версии библиотек и используемые модели).\n",
        "\n",
        "---\n",
        "\n",
        "## Альтернативный вариант (по согласованию с ведущим преподавателем)\n",
        "\n",
        "Если вы хотите использовать **другой датасет**, согласуйте его с преподавателем.  \n",
        "Требования к альтернативному датасету:\n",
        "- 1000–2000 объектов в бенчмарках;\n",
        "- наличие визуальной и текстовой информации;\n",
        "- возможность поставить **две разные задачи**:\n",
        "  1. одна решается с помощью метрик (классификация/регрессия),\n",
        "  2. другая оценивается через **LLM-as-a-Judge** или CLIPScore-подобные метрики.\n",
        "\n",
        "---\n",
        "\n",
        "## Критерии оценивания\n",
        "\n",
        "| Компонент | Баллы |\n",
        "|------------|-------|\n",
        "| Реализация и анализ классификации / регрессии | 2 |\n",
        "| Генерация и оценка текстов (Без использования LLM) | 2 |\n",
        "| Генерация и оценка текстов (LLM-as-Judge) | 2 |\n",
        "| Использование фреймворков (LMMs-Eval, etc) | по 1 баллу за каждый отдельный бенчмарк |\n",
        "| Анализ, визуализация, качество отчёта | 2 |\n",
        "\n",
        "Максимум: 10 баллов\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "d04IEuxeO9B7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "При необходимости, можно поднять собственный LLM-сервер в Colab, чтобы можно было использовать его для оценки качества и инференса с локальной машины."
      ],
      "metadata": {
        "id": "okqDzikpZPLt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Установка зависимостей"
      ],
      "metadata": {
        "id": "tnCCqX6eZqLx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_GS5tEWeO8OB"
      },
      "outputs": [],
      "source": [
        "!pip install vllm pyngrok fastapi uvicorn --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### Что такое ngrok\n",
        "**ngrok** — это инструмент, который позволяет создать **временный публичный доступ (туннель)**  \n",
        "к вашему локальному серверу, запущенному внутри Colab или на вашем компьютере.\n",
        "\n",
        "Colab-ноутбук работает в изолированной среде без внешнего IP-адреса.  \n",
        "Если мы поднимаем внутри него API-сервер (например, модель с OpenAI-совместимым API),  \n",
        "другие сервисы (или наши скрипты вне Colab) не смогут к нему подключиться напрямую.  \n",
        "\n",
        "**ngrok** решает эту проблему:\n",
        "- он “пробрасывает” локальный порт (например, `localhost:8000`) наружу,  \n",
        "- создаёт временный HTTPS-URL (например, `https://abc123.ngrok.io`),  \n",
        "- и перенаправляет запросы с этого URL внутрь Colab.\n",
        "\n",
        "*Пример:*  \n",
        "Вы запустили LLM-судью в Colab на порту `8000` → ngrok создаёт публичную ссылку,  \n",
        "которую можно использовать в коде как `openai.api_base = \"https://abc123.ngrok.io/v1\"`.  \n",
        "Теперь вашу LLM в Colab можно вызывать из внешних программ так же, как OpenAI API.\n"
      ],
      "metadata": {
        "id": "AXfzNrBRoX9T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Авторизация ngrok\n",
        "\n",
        "Зарегистрируйтесь на https://dashboard.ngrok.com/get-started/your-authtoken и вставьте свой токен:"
      ],
      "metadata": {
        "id": "E95MatedZ3Zz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "NGROK_AUTH_TOKEN = \"ваш_токен\"\n",
        "ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n"
      ],
      "metadata": {
        "id": "pLxtXQvzZyYp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Проброс наружу"
      ],
      "metadata": {
        "id": "d_TYyqUjZ8Is"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "public_url = ngrok.connect(8000, \"http\")\n",
        "print(f\"Публичный URL API: {public_url}\")"
      ],
      "metadata": {
        "id": "AQC497VXZ-RT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Вы увидите что-то вроде http://abcd1234.ngrok.io\n"
      ],
      "metadata": {
        "id": "aegaS77AaC8b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Что такое **vLLM**\n",
        "\n",
        "**vLLM** — это оптимизированный серверный движок для **быстрого инференса LLM**  \n",
        "(в том числе больших мультимодальных моделей вроде Qwen2.5-VL или LLaVA).\n",
        "- Загружает любую модель из Hugging Face (`transformers`) и запускает её как API-endpoint.  \n",
        "- Совместим с **OpenAI API-форматом**, т.е. можно обращаться к локальной модели через `openai.ChatCompletion.create(...)`.  \n",
        "- Поддерживает batch-инференс, streaming, CUDA-ускорение и управление памятью (PagedAttention).  \n",
        "- Может использоваться для нужного нам поднятия *локального LLM-судьи* прямо в Colab."
      ],
      "metadata": {
        "id": "c1va9bKIorjB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Запуск vLLM сервера (OpenAI-совместимого)"
      ],
      "metadata": {
        "id": "vd83yyneaIgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nohup python3 -m vllm.entrypoints.openai.api_server \\\n",
        "  --model Qwen/Qwen2-1.5B-Instruct \\\n",
        "  --host 0.0.0.0 \\\n",
        "  --port 8000 \\\n",
        "  > server.log 2>&1 &"
      ],
      "metadata": {
        "id": "moNO1wxUaK5p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Это займёт 1–2 минуты — vLLM подгрузит модель и запустит API. Смотрите на вывод в логах."
      ],
      "metadata": {
        "id": "hOOhreTbaNBy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Проверка, что сервер запущен"
      ],
      "metadata": {
        "id": "6iDWmXPBaTI5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl http://127.0.0.1:8000/v1/models"
      ],
      "metadata": {
        "id": "L9NY3Xs-aUhv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Увидете что-то вроде:\n",
        "\n",
        "`{\"object\":\"list\",\"data\":[{\"id\":\"Qwen/Qwen2-1.5B-Instruct\",\"object\":\"model\",\"created\":1761208147,\"owned_by\":\"vllm\",\"root\":\"Qwen/Qwen2-1.5B-Instruct\",\"parent\":null,\"max_model_len\":32768,\"permission\":[{\"id\":\"modelperm-ec699a8339664cee8b54aeaf323a7f4e\",\"object\":\"model_permission\",\"created\":1761208147,\"allow_create_engine\":false,\"allow_sampling\":true,\"allow_logprobs\":true,\"allow_search_indices\":false,\"allow_view\":true,\"allow_fine_tuning\":false,\"organization\":\"*\",\"group\":null,\"is_blocking\":false}]}]}`"
      ],
      "metadata": {
        "id": "Sm6cOBebafBu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Вызов API извне\n",
        "\n",
        "Теперь вы можете стучаться к серверу снаружи, как к OpenAI API:"
      ],
      "metadata": {
        "id": "HvOOEP-nan9Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "API_URL = \"http://abcd1234.ngrok.io/v1/chat/completions\"  #  замените URL\n",
        "headers = {\"Content-Type\": \"application/json\"}\n",
        "data = {\n",
        "    \"model\": \"Qwen/Qwen2-1.5B-Instruct\",\n",
        "    \"messages\": [\n",
        "        {\"role\": \"user\", \"content\": \"Привет! Расскажи шутку про ИИ.\"}\n",
        "    ],\n",
        "    \"temperature\": 0.7\n",
        "}\n",
        "\n",
        "r = requests.post(API_URL, headers=headers, json=data)\n",
        "print(r.json()[\"choices\"][0][\"message\"][\"content\"])\n"
      ],
      "metadata": {
        "id": "mdXzf0-wauJT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Использование как OpenAI клиента"
      ],
      "metadata": {
        "id": "SZcmsOaVa4mL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai --quiet"
      ],
      "metadata": {
        "id": "dh1aYMOQa-Xg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(\n",
        "    base_url=\"http://abcd1234.ngrok.io/v1\",\n",
        "    api_key=\"dummy\"  # vLLM не проверяет ключ\n",
        ")\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=\"Qwen/Qwen2-1.5B-Instruct\",\n",
        "    messages=[{\"role\": \"user\", \"content\": \"Расскажи о мультимодальных моделях простыми словами.\"}]\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)\n"
      ],
      "metadata": {
        "id": "4Q4qa3exbAJ9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}