{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJZyd2PKRT4Y"
      },
      "source": [
        "## Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KaIt3Gj_QrkY"
      },
      "outputs": [],
      "source": [
        "!pip install torch torchvision torchinfo open_clip_torch Pillow grad-cam"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4uC3Emm-RFSb"
      },
      "source": [
        "## CLIP ViT-B/32 PGD Cross-Entropy Untargeted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nS_qHFE3RRFf"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from torchvision import datasets\n",
        "from PIL import Image\n",
        "from torchinfo import summary\n",
        "from tqdm.auto import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "import open_clip\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "from pytorch_grad_cam import GradCAMPlusPlus"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Загрузим предобученную CLIP модель от OpenAI с ViT-B-32 энкодером изображений."
      ],
      "metadata": {
        "id": "kTMYxwEnDt20"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nkt51R-6GhC2"
      },
      "outputs": [],
      "source": [
        "device = \"cuda:0\"\n",
        "model, _, preprocess = open_clip.create_model_and_transforms(\n",
        "    \"ViT-B-32\", pretrained=\"openai\"\n",
        ")\n",
        "model = model.eval().to(device)\n",
        "tokenizer = open_clip.get_tokenizer(\"ViT-B-32\")\n",
        "summary(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Отделим нормализацию от остальных аугментаций, что позволит нам получать визуально похожие изображения, операция нормализации является дифференцируемой, поэтому можем ее использовать внутри PGD атаки."
      ],
      "metadata": {
        "id": "xNLdqfjjD1gu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LEhFJr8AYM1O"
      },
      "outputs": [],
      "source": [
        "normalize_transform = preprocess.transforms[-1]\n",
        "preprocess.transforms = preprocess.transforms[:-1]\n",
        "preprocess, normalize_transform"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Загрузим тест часть датасета CIFAR-10, датасет состоит из изображений объектов и их лейблов, всего 10 классов, изображения низкого разрешения 32x32."
      ],
      "metadata": {
        "id": "57UU-oTAEM_k"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VLF4RPzwSvgP"
      },
      "outputs": [],
      "source": [
        "N = 64\n",
        "batch_size = 64\n",
        "\n",
        "cifar10 = datasets.CIFAR10(\"./data\", train=False, download=True)\n",
        "\n",
        "images = [preprocess(Image.fromarray(cifar10.data[i]).convert(\"RGB\")) for i in range(N)]\n",
        "images = torch.stack(images)\n",
        "labels = torch.tensor([cifar10.targets[i] for i in range(len(images))])\n",
        "\n",
        "dataset = TensorDataset(images, labels)\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "print(images.shape, labels.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P16YG-M6XR7Y"
      },
      "outputs": [],
      "source": [
        "cifar10.idx_to_class = {v: k for k, v in cifar10.class_to_idx.items()}\n",
        "cifar10.idx_to_class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y4nu-H9LUKw6"
      },
      "outputs": [],
      "source": [
        "nrows, ncols, figsize = 4, 8, (15, 10)\n",
        "fig, axes = plt.subplots(nrows, ncols, figsize=figsize)\n",
        "for i, ax in enumerate(axes.flat):\n",
        "    img = images[i].cpu().permute(1, 2, 0).numpy()\n",
        "    ax.imshow(img)\n",
        "    ax.set_title(f\"{cifar10.idx_to_class[labels[i].item()]}\")\n",
        "    ax.axis(\"off\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Для CLIP в zero-shot классификации будем использовать текстовые названия классов, text_features - 10 текстовых эмбеддингов размера 512, соответствующих названиям классов."
      ],
      "metadata": {
        "id": "uPZ9o-k0EkPh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "57ppxGjsTE6J"
      },
      "outputs": [],
      "source": [
        "text_inputs = torch.cat([tokenizer(l) for l in cifar10.idx_to_class.values()]).to(\n",
        "    device\n",
        ")\n",
        "with torch.no_grad():\n",
        "    text_features = model.encode_text(text_inputs)\n",
        "    text_features = F.normalize(text_features, dim=-1)\n",
        "text_features.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PGD - white-box атака, которая итеративно с шагом в $\\alpha$ делает шаги по направлению градиента к входным данным, изменение значений модифицируемых примеров ограничено сферой $S$ размера $\\epsilon$.\n",
        "$$x^{t+1} = \\prod_{x+S}(x^t + \\alpha \\cdot sign(\\nabla_xL(\\theta, x, y)))$$\n",
        "\n",
        "В качестве функции потерь будем оптимизировать классификационную функцию потерь (в нашем случае Cross-Entropy), причем в ненаправленном варианте мы стараемся повысить значение функции потерь между предсказанием и реальными метками."
      ],
      "metadata": {
        "id": "mGzGWDMTFDQQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "wMP3GJPuTZjG"
      },
      "outputs": [],
      "source": [
        "def pgd_attack(dataloader, text_features, eps=0.1, alpha=1.0 / 255.0, steps=100):\n",
        "    all_adv = []\n",
        "\n",
        "    for batch_idx, (batch_images, batch_labels) in enumerate(dataloader):\n",
        "        batch_images, batch_labels = batch_images.to(device), batch_labels.to(device)\n",
        "        delta = torch.zeros_like(batch_images, requires_grad=True)\n",
        "\n",
        "        pbar = tqdm(range(steps))\n",
        "        for _ in pbar:\n",
        "            adv_images = normalize_transform(torch.clamp(batch_images + delta, 0, 1))\n",
        "            image_features = F.normalize(model.encode_image(adv_images), dim=-1)\n",
        "\n",
        "            logits = image_features @ text_features.T\n",
        "\n",
        "            loss = F.cross_entropy(logits, batch_labels)\n",
        "            loss.backward()\n",
        "\n",
        "            delta.data = delta.data + alpha * delta.grad.sign()\n",
        "            delta.data = torch.clamp(delta.data, -eps, eps)\n",
        "            delta.grad.zero_()\n",
        "\n",
        "            pbar.set_description(\n",
        "                f\"Batch = {batch_idx+1} / {len(dataloader)}, loss = {loss.item():.3f}\"\n",
        "            )\n",
        "\n",
        "        all_adv.append(torch.clamp(batch_images + delta, 0, 1).detach().cpu())\n",
        "\n",
        "    return torch.cat(all_adv)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Получим тензор значений для adversarial примеров после PGD атаки с $\\alpha = \\frac{1}{255}$, $\\epsilon = 0.1$ и $t = 100$."
      ],
      "metadata": {
        "id": "-_6iDyY2HZFh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r2R_Sd7UTmPy"
      },
      "outputs": [],
      "source": [
        "adversarial_images = pgd_attack(dataloader, text_features, alpha=1.0 / 255.0)\n",
        "adversarial_images.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Функция для расчета классических классификационных метрик - accuracy, precision, recall, f1_score."
      ],
      "metadata": {
        "id": "MfNYWmnnHtBI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QaV4uUiSbSX0"
      },
      "outputs": [],
      "source": [
        "def calc_metrics(y_true, y_pred):\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    prec, rec, f1, _ = precision_recall_fscore_support(\n",
        "        y_true, y_pred, average=\"macro\", zero_division=0\n",
        "    )\n",
        "    return acc, prec, rec, f1\n",
        "\n",
        "\n",
        "def print_metrics(real_labels, orig_labels, adv_labels):\n",
        "    y_true = real_labels.cpu().numpy()\n",
        "\n",
        "    orig_pred = orig_labels.cpu().numpy()\n",
        "    adv_pred = adv_labels.cpu().numpy()\n",
        "\n",
        "    orig_metrics = calc_metrics(y_true, orig_pred)\n",
        "    adv_metrics = calc_metrics(y_true, adv_pred)\n",
        "\n",
        "    print(f\"{'Metric':<12} {'Original':>10} {'Adversarial':>12}\")\n",
        "    print(\"-\" * 36)\n",
        "    for name, orig, adv in zip(\n",
        "        [\"Accuracy\", \"Precision\", \"Recall\", \"F1-Score\"], orig_metrics, adv_metrics\n",
        "    ):\n",
        "        print(f\"{name:<12} {orig:>10.2%} {adv:>12.2%}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lIQK70Nwkvj2"
      },
      "outputs": [],
      "source": [
        "orig_dataset = TensorDataset(images)\n",
        "adv_dataset = TensorDataset(adversarial_images)\n",
        "\n",
        "orig_dataloader = DataLoader(orig_dataset, batch_size=batch_size, shuffle=False)\n",
        "adv_dataloader = DataLoader(adv_dataset, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Функция для получения предсказаний, ищем наиболее близкий эмбеддинг класса по косинусной схожести к эмбеддингу изображения."
      ],
      "metadata": {
        "id": "GBbaHwsKH7Bk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pQrJMImWbuQN"
      },
      "outputs": [],
      "source": [
        "def get_labels(orig_dataloader, adv_dataloader, text_features):\n",
        "    with torch.no_grad():\n",
        "        orig_labels, adv_labels = [], []\n",
        "\n",
        "        for orig_images_batch, adv_images_batch in tqdm(\n",
        "            zip(orig_dataloader, adv_dataloader), total=len(orig_dataloader)\n",
        "        ):\n",
        "            orig_images_batch, adv_images_batch = orig_images_batch[0].to(\n",
        "                device\n",
        "            ), adv_images_batch[0].to(device)\n",
        "\n",
        "            orig_features = F.normalize(model.encode_image(orig_images_batch), dim=-1)\n",
        "            adv_features = F.normalize(model.encode_image(adv_images_batch), dim=-1)\n",
        "\n",
        "            orig_labels.append((orig_features @ text_features.T).argmax(dim=-1))\n",
        "            adv_labels.append((adv_features @ text_features.T).argmax(dim=-1))\n",
        "\n",
        "        orig_labels = torch.concatenate(orig_labels, dim=0)\n",
        "        adv_labels = torch.concatenate(adv_labels, dim=0)\n",
        "\n",
        "    return orig_labels, adv_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EoNbWPIulyjw"
      },
      "outputs": [],
      "source": [
        "orig_labels, adv_labels = get_labels(orig_dataloader, adv_dataloader, text_features)\n",
        "print_metrics(labels, orig_labels, adv_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UFPFyDyBU_xs"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(nrows, ncols, figsize=figsize)\n",
        "for i, ax in enumerate(axes.flat):\n",
        "    img = adversarial_images[i].cpu().permute(1, 2, 0).numpy()\n",
        "    ax.imshow(img)\n",
        "    ax.set_title(f\"{cifar10.idx_to_class[adv_labels[i].item()]}\")\n",
        "    ax.axis(\"off\")\n",
        "plt.suptitle(\"Adversarial\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MCog6zqiTruA"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(nrows, ncols, figsize=figsize)\n",
        "for i, ax in enumerate(axes.flat):\n",
        "    img = images[i].cpu().permute(1, 2, 0).numpy()\n",
        "    ax.imshow(img)\n",
        "    ax.set_title(f\"{cifar10.idx_to_class[orig_labels[i].item()]}\")\n",
        "    ax.axis(\"off\")\n",
        "plt.suptitle(\"Clean\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Вспомогательные функции для GradCAM+"
      ],
      "metadata": {
        "id": "KAwuYxpiIIGL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GmWKfwDIMHu6"
      },
      "outputs": [],
      "source": [
        "def make_vit_reshape_transform(model):\n",
        "    img_size = getattr(model.visual, \"image_size\", 224)\n",
        "    if isinstance(img_size, (tuple, list)):\n",
        "        img_h = img_size[0]\n",
        "        img_w = img_size[1] if len(img_size) > 1 else img_size[0]\n",
        "    else:\n",
        "        img_h = img_w = int(img_size)\n",
        "\n",
        "    patch = getattr(model.visual, \"patch_size\", 32)\n",
        "    if isinstance(patch, (tuple, list)):\n",
        "        ph = patch[0]\n",
        "        pw = patch[1] if len(patch) > 1 else patch[0]\n",
        "    else:\n",
        "        ph = pw = int(patch)\n",
        "\n",
        "    h = img_h // ph\n",
        "    w = img_w // pw\n",
        "\n",
        "    def _reshape_transform(tensor):\n",
        "        b, _, c = tensor.shape\n",
        "        patches = tensor[:, 1:, :].reshape(b, h, w, c)\n",
        "        return patches.permute(0, 3, 1, 2).contiguous()\n",
        "\n",
        "    return _reshape_transform\n",
        "\n",
        "\n",
        "class CLIPImageWrapper(nn.Module):\n",
        "    def __init__(self, clip_model):\n",
        "        super().__init__()\n",
        "        self.clip = clip_model\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.clip.encode_image(x)\n",
        "\n",
        "\n",
        "class CLIPTextTarget:\n",
        "    def __init__(self, text_features):\n",
        "        self.text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "    def __call__(self, image_features):\n",
        "        image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
        "        return image_features @ self.text_features.T\n",
        "\n",
        "\n",
        "def run_clip_gradcam(gradcam, image_tensor, text_features):\n",
        "    model.eval()\n",
        "    if hasattr(model, \"visual\"):\n",
        "        model.visual.float()\n",
        "\n",
        "    targets = [CLIPTextTarget(text_features)]\n",
        "\n",
        "    grayscale_cam = gradcam(\n",
        "        input_tensor=image_tensor, targets=targets, eigen_smooth=True\n",
        "    )[0]\n",
        "    return grayscale_cam\n",
        "\n",
        "\n",
        "def visualize_gradcam(gradcam, images, adv_images, labels, n):\n",
        "    _, ax = plt.subplots(nrows=n, ncols=4, figsize=(12, 3 * n))\n",
        "\n",
        "    for i in range(n):\n",
        "        class_idx = labels[i].item()\n",
        "\n",
        "        mask_clean_i = run_clip_gradcam(\n",
        "            gradcam, images[i : i + 1], text_features[class_idx : class_idx + 1]\n",
        "        )\n",
        "        mask_adv_i = run_clip_gradcam(\n",
        "            gradcam, adv_images[i : i + 1], text_features[class_idx : class_idx + 1]\n",
        "        )\n",
        "\n",
        "        img = images[i].cpu().permute(1, 2, 0).numpy()\n",
        "        adv_img = adv_images[i].cpu().permute(1, 2, 0).numpy()\n",
        "\n",
        "        ax[i][0].imshow(img)\n",
        "        ax[i][0].set_xlabel(\n",
        "            f\"Clean image with label = {cifar10.idx_to_class[class_idx]}\"\n",
        "        )\n",
        "\n",
        "        ax[i][1].imshow(mask_clean_i, cmap=\"jet\")\n",
        "        ax[i][1].set_xlabel(\"Clean GradCAM+\")\n",
        "\n",
        "        ax[i][2].imshow(adv_img)\n",
        "        ax[i][2].set_xlabel(\"Adversarial image\")\n",
        "\n",
        "        ax[i][3].imshow(mask_adv_i, cmap=\"jet\")\n",
        "        ax[i][3].set_xlabel(\"Adversarial GradCAM+\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Отрисуем GradCAM+ heatmap для слоя LayerNorm с последнего блока ViT.\n",
        "\n",
        "GradCAM вычисляет важность каждой активации в заданном слое для предсказания определенного класса, используя градиенты функции потерь по этим активациям в качестве весов."
      ],
      "metadata": {
        "id": "Dxjc4uNDIN4G"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HgnnTUZ0MSrw"
      },
      "outputs": [],
      "source": [
        "target_layer = model.visual.transformer.resblocks[-1].ln_1\n",
        "reshape_transform = make_vit_reshape_transform(model)\n",
        "\n",
        "gradcam = GradCAMPlusPlus(\n",
        "    model=CLIPImageWrapper(model),\n",
        "    target_layers=[target_layer],\n",
        "    reshape_transform=reshape_transform,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AGQko2JRMsaj"
      },
      "outputs": [],
      "source": [
        "visualize_gradcam(gradcam, images, adversarial_images, labels, 10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQh73HgnctDj"
      },
      "source": [
        "## CLIP ViT-B/32 PGD Cross-Entropy Targeted"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Добавим новый 11 класс, который хотим использовать как целевой для атаки, назовем его так, чтобы модель на чистых примерах не притягивала его в большинстве случаев."
      ],
      "metadata": {
        "id": "p6Hsk1_uJPyH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g5RuK_u2cue_"
      },
      "outputs": [],
      "source": [
        "cifar10.class_to_idx[\"<UNKNOWN_TARGET>\"] = 10\n",
        "cifar10.idx_to_class = {v: k for k, v in cifar10.class_to_idx.items()}\n",
        "cifar10.idx_to_class"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Теперь скажем, что целевые метки - это наш новый класс."
      ],
      "metadata": {
        "id": "Dj30bFGXJmLH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sQQpTecpewh4"
      },
      "outputs": [],
      "source": [
        "target_labels = torch.tensor([10 for _ in range(len(images))])\n",
        "\n",
        "dataset = TensorDataset(images, target_labels)\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "images.shape, target_labels.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b-u6WrP7dVIT"
      },
      "outputs": [],
      "source": [
        "text_inputs = torch.cat([tokenizer(l) for l in cifar10.idx_to_class.values()])\n",
        "text_inputs = text_inputs.to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    text_features = model.encode_text(text_inputs)\n",
        "    text_features = F.normalize(text_features, dim=-1)\n",
        "\n",
        "text_features.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Модифицируем функцию потерь для PGD, теперь мы хотим на самом деле минимизировать функцию потерь, т.к. целевой класс у нас неверный (наш новый целевой класс), просто добавим знак \"-\" перед Cross-Entropy для изменения направления оптимизации."
      ],
      "metadata": {
        "id": "wKk8yS9HJf_g"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BdKJbI5AdX8j"
      },
      "outputs": [],
      "source": [
        "def pgd_attack(dataloader, text_features, eps=0.1, alpha=1.0 / 255.0, steps=100):\n",
        "    all_adv = []\n",
        "\n",
        "    for batch_idx, (batch_images, batch_labels) in enumerate(dataloader):\n",
        "        batch_images, batch_labels = batch_images.to(device), batch_labels.to(device)\n",
        "        delta = torch.zeros_like(batch_images, requires_grad=True)\n",
        "\n",
        "        pbar = tqdm(range(steps))\n",
        "        for _ in pbar:\n",
        "            adv_images = normalize_transform(torch.clamp(batch_images + delta, 0, 1))\n",
        "            image_features = F.normalize(model.encode_image(adv_images), dim=-1)\n",
        "\n",
        "            logits = image_features @ text_features.T\n",
        "\n",
        "            loss = -1.0 * F.cross_entropy(logits, batch_labels)\n",
        "            loss.backward()\n",
        "\n",
        "            delta.data = delta.data + alpha * delta.grad.sign()\n",
        "            delta.data = torch.clamp(delta.data, -eps, eps)\n",
        "            delta.grad.zero_()\n",
        "\n",
        "            pbar.set_description(\n",
        "                f\"Batch = {batch_idx+1} / {len(dataloader)}, loss = {loss.item():.3f}\"\n",
        "            )\n",
        "\n",
        "        all_adv.append(torch.clamp(batch_images + delta, 0, 1).detach().cpu())\n",
        "\n",
        "    return torch.cat(all_adv)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "02cwqqvsdtu9"
      },
      "outputs": [],
      "source": [
        "adversarial_images = pgd_attack(dataloader, text_features, alpha=1.0 / 255.0)\n",
        "adversarial_images.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9lTcpyuCnkHl"
      },
      "outputs": [],
      "source": [
        "adv_dataset = TensorDataset(adversarial_images)\n",
        "adv_dataloader = DataLoader(adv_dataset, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h-kmgyb1onyy"
      },
      "outputs": [],
      "source": [
        "orig_labels, adv_labels = get_labels(orig_dataloader, adv_dataloader, text_features)\n",
        "print_metrics(labels, orig_labels, adv_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "68ylPFgcop0q"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(nrows, ncols, figsize=figsize)\n",
        "for i, ax in enumerate(axes.flat):\n",
        "    img = adversarial_images[i].cpu().permute(1, 2, 0).numpy()\n",
        "    ax.imshow(img)\n",
        "    ax.set_title(f\"{cifar10.idx_to_class[adv_labels[i].item()]}\")\n",
        "    ax.axis(\"off\")\n",
        "plt.suptitle(\"Adversarial\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2HSdCeeCpCJb"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(nrows, ncols, figsize=figsize)\n",
        "for i, ax in enumerate(axes.flat):\n",
        "    img = images[i].cpu().permute(1, 2, 0).numpy()\n",
        "    ax.imshow(img)\n",
        "    ax.set_title(f\"{cifar10.idx_to_class[orig_labels[i].item()]}\")\n",
        "    ax.axis(\"off\")\n",
        "plt.suptitle(\"Clean\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3rHcc_KHNBzy"
      },
      "outputs": [],
      "source": [
        "visualize_gradcam(gradcam, images, adversarial_images, labels, 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "es1nrh5wOU0O"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}